{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Q_learning_example.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMqBpbOQoJUKfKyXFB4g4i6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliyah-smith/reinforcement_learning_tutorial/blob/master/Q_learning_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQV1rGXExGsa",
        "colab_type": "text"
      },
      "source": [
        "This tutorial will show how to implement a Q-learning algorithm for two simple toy problems. Comment out the necessary code below to run one of the games.\n",
        "\n",
        "\n",
        "> The frozen lake environment is a 4x4 grid, so there is a discrete state space \n",
        "as well as a discrete actions space i.e. the agent can move up, down, left, or right to one of the 16 squares within the grid. The goal of the game is to move from the start block to the end block without falling into one of the dangerous holes. There is a +1 reward for reaching the goal and 0 reward otherwise.\n",
        "\n",
        "\n",
        "> The taxi environment is a 5x5 grid. There are four marked locations in the grab and the taxi learns to pick up a passenger at one of the locations and drop them off at another location in the shortest amount of time. There is a +20 reward for a successful drop off, -10 penalty for illegal pick up and drop off actions, and -1 penalty at every timestep.\n",
        "\n",
        "\n",
        "\n",
        "This tutorial is modified from Arthur Juliani, \"Simple Reinforcement Learning with Tensor Flow\", 2016."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fop1mgxYhFSQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym \n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "\n",
        "# Load OpenAI Frozen Lake environment\n",
        "#env = gym.make('FrozenLake-v0')\n",
        "env = gym.make('Taxi-v3')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpCAUuWyyzhg",
        "colab_type": "text"
      },
      "source": [
        "**Initialize Q-table**\n",
        "> The Q table keeps track of the Q-value (long-term expected reward stemming from a state s and action a) for each state-action pair. In this case, the Q table will be 16 x 4 for the 16 states and 4 actions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZnYDMsDTyxjI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Q = np.zeros([env.observation_space.n,env.action_space.n])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTGht71GyRj-",
        "colab_type": "text"
      },
      "source": [
        "**Set the parameters**\n",
        "\n",
        "*   learning rate: value between 0 and 1 which controls how fast learning occurs (0 - nothing is learned, closer to 1 - learning occurs quickly)\n",
        "*   discount factor (gamma): value between 0 and 1 which controls whether immediate or future rewards are prioritized (0 - imemdiate rewards are important, closer to 1 - future rewards are important)\n",
        "*   epsilon: paramter for epsilon-greedy policy - epsilon is the probability that a random action will be taken\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC91zyD3yN5M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Frozen Lake Parameters\n",
        "#learning_rate = 0.6\n",
        "#gamma = 0.95\n",
        "\n",
        "# Taxi Parameters\n",
        "learning_rate = 0.1 \n",
        "gamma = 0.6 \n",
        "epsilon = 0.1\n",
        "\n",
        "# Set Number of episodes\n",
        "episodes = 2000\n",
        "\n",
        "# Initialize lists that will store rewards and steps per episode\n",
        "rewards = []\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewx2VltLxn7D",
        "colab_type": "text"
      },
      "source": [
        "**Action Policy**\n",
        "Set up the Q-learning algorithm with one of two action policies to leverage exploration and exploitation\n",
        "\n",
        "1.   Action that maximizes Q + noise - The parameters are optimized for this method for the Frozen Lake game\n",
        "2.   Epsilon-greedy policy - The parameters are optimized for this method for the Taxi game\n",
        "\n",
        "The epsilon-greedy policy: With probability epsilon, the agent performs a random action. With probability 1-epsilon, the agent choose the action the maximizes Q.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkwWhCF3xh5O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For loop for each episode\n",
        "for i in range(episodes):\n",
        "    # Reset the environment and perform first observation\n",
        "    s = env.reset()\n",
        "    rewards_all = 0\n",
        "    d = False\n",
        "    j = 0\n",
        "\n",
        "    # Q-learning Algorithm \n",
        "    while j < 99:\n",
        "        j+=1\n",
        "        # Perform action with noise (which encourages exploration) - Frozen Lake\n",
        "        #a = np.argmax(Q[s,:]+ np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
        "        # OR Perform action - epsilon-greedy policy - Taxi\n",
        "        if random.uniform(0,1) < epsilon:\n",
        "            a = env.action_space.sample()\n",
        "        else:\n",
        "            a = np.argmax(Q[s,:])\n",
        "        \n",
        "        # Get new state and reward from environment\n",
        "        s_new,r,d,_ = env.step(a)\n",
        "\n",
        "        # Update Q-table\n",
        "        Q[s,a] = Q[s,a] + learning_rate*(r+gamma*np.max(Q[s_new,:]) - Q[s,a])\n",
        "        rewards_all += r\n",
        "        s = s_new\n",
        "        if d == True:\n",
        "            break\n",
        "        rewards.append(rewards_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIKbkahpxekk",
        "colab_type": "text"
      },
      "source": [
        "Print out the final Q-table"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5PlFwb5xcyi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "outputId": "1f570fdf-e035-4a83-d71b-9621b4a764b8"
      },
      "source": [
        "print(\"Score over time: \", sum(rewards)/episodes)\n",
        "print(\"Final Q-table Values\")\n",
        "print(Q)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score over time:  -3661.3525\n",
            "Final Q-table Values\n",
            "[[ 0.          0.          0.          0.          0.          0.        ]\n",
            " [-2.29593199 -2.29870647 -2.2876985  -2.30060569 -2.27913591 -6.82796348]\n",
            " [-1.73443868 -1.75961522 -1.80314659 -1.73494657 -0.7671085  -6.13238327]\n",
            " ...\n",
            " [-1.19403701 -0.8711333  -1.18986086 -1.23611754 -1.96       -1.96      ]\n",
            " [-1.98750763 -2.00167605 -1.99042537 -1.991924   -3.68759462 -5.04540497]\n",
            " [-0.196      -0.2878     -0.196       3.8686428  -1.         -1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QREY1hpxYKv",
        "colab_type": "text"
      },
      "source": [
        "Evaluate the agent's performance after Q-learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s77KaaJoxTwX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bbc07d89-59d3-4a2c-939c-aecf42e258a6"
      },
      "source": [
        "j = 0\n",
        "#epsilon = 0.05\n",
        "s = env.reset()\n",
        "rewards_all = 0\n",
        "d = False\n",
        "j = 0\n",
        "while j < 99:\n",
        "    env.render()\n",
        "    print(\"Episode: \",j)\n",
        "    j+=1\n",
        "    # Perform action with noise - Frozen Lake\n",
        "    #a = np.argmax(Q[s,:]+ np.random.randn(1,env.action_space.n)*(1./(i+1)))\n",
        "    # OR Perform action - epsilon-greedy policy - Taxi\n",
        "    if random.uniform(0,1) < epsilon:\n",
        "        a = env.action_space.sample()\n",
        "    else:\n",
        "        a = np.argmax(Q[s,:])\n",
        "        \n",
        "    # Get new state and reward from environment\n",
        "    s_new,r,d,_ = env.step(a)\n",
        "\n",
        "    # Update Q-table\n",
        "    Q[s,a] = Q[s,a] + learning_rate*(r+gamma*np.max(Q[s_new,:]) - Q[s,a])\n",
        "    rewards_all += r\n",
        "    s = s_new\n",
        "    if d == True:\n",
        "            break\n",
        "    rewards.append(rewards_all)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n",
            "Episode:  0\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : |\u001b[43m \u001b[0m: |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "Episode:  1\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |\u001b[43mB\u001b[0m: |\n",
            "+---------+\n",
            "  (South)\n",
            "Episode:  2\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
            "+---------+\n",
            "  (East)\n",
            "Episode:  3\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
            "+---------+\n",
            "  (South)\n",
            "Episode:  4\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B:\u001b[43m \u001b[0m|\n",
            "+---------+\n",
            "  (East)\n",
            "Episode:  5\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | :\u001b[43m \u001b[0m|\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Episode:  6\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : :\u001b[43m \u001b[0m|\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Episode:  7\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : :\u001b[43m \u001b[0m|\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Episode:  8\n",
            "+---------+\n",
            "|R: | : :\u001b[34;1m\u001b[43mG\u001b[0m\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Episode:  9\n",
            "+---------+\n",
            "|R: | : :\u001b[42mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (Pickup)\n",
            "Episode:  10\n",
            "+---------+\n",
            "|R: | :\u001b[42m_\u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "Episode:  11\n",
            "+---------+\n",
            "|R: | :\u001b[42m_\u001b[0m:G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (North)\n",
            "Episode:  12\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | :\u001b[42m_\u001b[0m: |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "Episode:  13\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : :\u001b[42m_\u001b[0m: |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "Episode:  14\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : :\u001b[42m_\u001b[0m: : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "Episode:  15\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[42m_\u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "Episode:  16\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "|\u001b[42m_\u001b[0m: : : : |\n",
            "| | : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (West)\n",
            "Episode:  17\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "|\u001b[42m_\u001b[0m| : | : |\n",
            "|\u001b[35mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "Episode:  18\n",
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[35m\u001b[42mY\u001b[0m\u001b[0m| : |B: |\n",
            "+---------+\n",
            "  (South)\n",
            "Episode:  19\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuFQzBOOxOAE",
        "colab_type": "text"
      },
      "source": [
        "The agent has learned how to successfully reach the final destination."
      ]
    }
  ]
}