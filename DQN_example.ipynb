{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DQN_example.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMxVxAOF699bvykXfXSaOt9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/aliyah-smith/reinforcement_learning_tutorial/blob/master/DQN_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MuO7UTIOD6u0",
        "colab_type": "text"
      },
      "source": [
        "In this example, we set up the DQN algorithm, a deep q-learning technique for reiforcement learning. This algorithm is tested in two OpenAI Gym environments: Frozen Lake and Cart Pole.\n",
        "\n",
        "\n",
        "> In the Frozen Lake problem, we train an agent that learns how to move from a start block to an end block in a 4 x 4 grid, while avoiding obstacles.\n",
        "\n",
        "> In the Cart Pole problem, we train an agent that learns how to balance a cartpole in the upright position by applying horizontal forces to the pivot point. This is achieved using DQN.\n",
        "\n",
        "This code is adapted and modified from Siwei Xu, \"Deep Reinforcement Learning: Build a Deep Q-network(DQN) with TensorFlow 2\", 2019."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvrblxTGKbwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import gym\n",
        "import os\n",
        "import datetime\n",
        "from statistics import mean\n",
        "from gym import wrappers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FyPH6iLnD1jz",
        "colab_type": "text"
      },
      "source": [
        "Define Neural Network layers in the class for the game model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PTT_LZlcDznQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self, num_states, hidden_units, num_actions):\n",
        "        super(MyModel, self).__init__()\n",
        "        # Define input layer - entry point into network\n",
        "        self.input_layer = tf.keras.layers.InputLayer(input_shape=(num_states,))\n",
        "        # Initialize hidden layers - layers between input and output (where computation is done)\n",
        "        self.hidden_layers = []\n",
        "        for i in hidden_units:\n",
        "            # Define the units within densely-connected NN layer\n",
        "            self.hidden_layers.append(tf.keras.layers.Dense(\n",
        "                i, activation='tanh', kernel_initializer='RandomNormal'))\n",
        "        # Define output layer - result for given inputs\n",
        "        self.output_layer = tf.keras.layers.Dense(\n",
        "            num_actions, activation='linear', kernel_initializer='RandomNormal')\n",
        "\n",
        "    @tf.function #Compiles a function into a callable TensorFlow graph\n",
        "    def call(self, inputs):\n",
        "        z = self.input_layer(inputs)\n",
        "        for layer in self.hidden_layers:\n",
        "            z = layer(z)\n",
        "        output = self.output_layer(z)\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gp2XOKjHDwKt",
        "colab_type": "text"
      },
      "source": [
        "Define DQN class\n",
        "\n",
        "\n",
        "> **Training** \n",
        "\n",
        "\n",
        "> Train the network parameters by:\n",
        "\n",
        "\n",
        "1.   sampling from experience\n",
        "2.   calculating the predicted values\n",
        "3.   determining the loss between the target and actual networks\n",
        "4.   minimizing the loss \n",
        "\n",
        "\n",
        "\n",
        "> **Select & Perform an Action**\n",
        "\n",
        "\n",
        "> The epsilon-greedy policy is used to determine an action\n",
        "\n",
        "\n",
        "\n",
        "> **Experience Replay Buffer**\n",
        "\n",
        "\n",
        "\n",
        "> **Copy Weights to Target Network**\n",
        "\n",
        "\n",
        "> After C iterations, the weights from the actual network are copied over to the target network and training continues\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B5CalqgQDtxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "class DQN:\n",
        "    def __init__(self, num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr):\n",
        "        self.num_actions = num_actions #number of possible actions\n",
        "        self.batch_size = batch_size #size of batch sampled from experience\n",
        "        self.optimizer = tf.optimizers.Adagrad(lr) #optimizer\n",
        "        self.gamma = gamma #discount rate\n",
        "        self.model = MyModel(num_states, hidden_units, num_actions)\n",
        "        self.experience = {'s': [], 'a': [], 'r': [], 's2': [], 'done': []} #experince tuple = (state, action, reward, next state)\n",
        "        self.max_experiences = max_experiences\n",
        "        self.min_experiences = min_experiences\n",
        "\n",
        "    def predict(self, inputs):\n",
        "        return self.model(np.atleast_2d(inputs.astype('float32')))\n",
        "\n",
        "    def train(self, TargetNet):\n",
        "        if len(self.experience['s']) < self.min_experiences:\n",
        "            return 0\n",
        "        # Sample from experience\n",
        "        ids = np.random.randint(low=0, high=len(self.experience['s']), size=self.batch_size)\n",
        "        states = np.asarray([self.experience['s'][i] for i in ids])\n",
        "        actions = np.asarray([self.experience['a'][i] for i in ids])\n",
        "        rewards = np.asarray([self.experience['r'][i] for i in ids])\n",
        "        states_next = np.asarray([self.experience['s2'][i] for i in ids])\n",
        "        dones = np.asarray([self.experience['done'][i] for i in ids])\n",
        "        \n",
        "        # Get the values at next state\n",
        "        value_next = np.max(TargetNet.predict(states_next), axis=1)\n",
        "        actual_values = np.where(dones, rewards, rewards+self.gamma*value_next)\n",
        "\n",
        "        # Calculate squared loss between actual & target Q-values and Perform Gradient Descent\n",
        "        with tf.GradientTape() as tape:\n",
        "            selected_action_values = tf.math.reduce_sum(\n",
        "                self.predict(states) * tf.one_hot(actions, self.num_actions), axis=1)\n",
        "            loss = tf.math.reduce_mean(tf.square(actual_values - selected_action_values))\n",
        "        variables = self.model.trainable_variables\n",
        "        gradients = tape.gradient(loss, variables)\n",
        "        self.optimizer.apply_gradients(zip(gradients, variables))\n",
        "        return loss #this is a loss tensor\n",
        "\n",
        "    def get_action(self, states, epsilon):\n",
        "        if np.random.random() < epsilon:\n",
        "            return np.random.choice(self.num_actions)\n",
        "        else:\n",
        "            return np.argmax(self.predict(np.atleast_2d(states))[0])\n",
        "    \n",
        "    def add_experience(self, exp):\n",
        "        if len(self.experience['s']) >= self.max_experiences:\n",
        "            for key in self.experience.keys():\n",
        "                self.experience[key].pop(0)\n",
        "        for key, value in exp.items():\n",
        "            self.experience[key].append(value)\n",
        "\n",
        "    def copy_weights(self, TrainNet):\n",
        "        variables1 = self.model.trainable_variables\n",
        "        variables2 = TrainNet.model.trainable_variables\n",
        "        for v1, v2 in zip(variables1, variables2):\n",
        "            v1.assign(v2.numpy())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-cPCGkFDSmB",
        "colab_type": "text"
      },
      "source": [
        "**Play a game**\n",
        "\n",
        "\n",
        "> Define function to play a game"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7ch_JwqDPz7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def play_game(env, TrainNet, TargetNet, epsilon, copy_step):\n",
        "    rewards = 0\n",
        "    iter = 0\n",
        "    done = False\n",
        "    observations = env.reset()\n",
        "    losses = list()\n",
        "    while not done:\n",
        "        action = TrainNet.get_action(observations, epsilon)\n",
        "        prev_observations = observations\n",
        "        observations, reward, done, _ = env.step(action)\n",
        "        rewards += reward\n",
        "        if done:\n",
        "            reward = -200\n",
        "            env.reset()\n",
        "\n",
        "        exp = {'s': prev_observations, 'a': action, 'r': reward, 's2': observations, 'done': done}\n",
        "        TrainNet.add_experience(exp)\n",
        "        loss = TrainNet.train(TargetNet)\n",
        "        if isinstance(loss, int):\n",
        "            losses.append(loss)\n",
        "        else:\n",
        "            losses.append(loss.numpy())\n",
        "        iter += 1\n",
        "        if iter % copy_step == 0:\n",
        "            TargetNet.copy_weights(TrainNet)\n",
        "    return rewards, mean(losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIqSYh9fDOxz",
        "colab_type": "text"
      },
      "source": [
        "**Create Visualization**\n",
        "\n",
        "\n",
        "> Create a video to show how the game works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LbAPvisDGKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def make_video(env, TrainNet):\n",
        "    env = wrappers.Monitor(env, os.path.join(os.getcwd(), \"videos\"), force=True)\n",
        "    rewards = 0\n",
        "    steps = 0\n",
        "    done = False\n",
        "    observation = env.reset()\n",
        "    while not done:\n",
        "        env.render()\n",
        "        action = TrainNet.get_action(observation, 0)\n",
        "        observation, reward, done, _ = env.step(action)\n",
        "        steps += 1\n",
        "        rewards += reward\n",
        "    print(\"Testing steps: {} rewards {}: \".format(steps, rewards))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34TyIi9TDFG0",
        "colab_type": "text"
      },
      "source": [
        "**Putting everything together**\n",
        "\n",
        "\n",
        "> Define the function to import the environment (Frozen Lake or Cart Pole) and begin learning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rDJ1SI2KC7D5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    env = gym.make('CartPole-v0')\n",
        "    #env = gym.make('FrozenLake-v0')\n",
        "\n",
        "    #Define parameters\n",
        "    gamma = 0.99\n",
        "    copy_step = 25\n",
        "    num_states = len(env.observation_space.sample())\n",
        "    num_actions = env.action_space.n\n",
        "    hidden_units = [200, 200]\n",
        "    max_experiences = 10000\n",
        "    min_experiences = 100\n",
        "    batch_size = 32\n",
        "    lr = 1e-2\n",
        "    current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "    log_dir = 'logs/dqn/' + current_time\n",
        "    summary_writer = tf.summary.create_file_writer(log_dir)\n",
        "\n",
        "    #Start training target & actual networks\n",
        "    TrainNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
        "    TargetNet = DQN(num_states, num_actions, hidden_units, gamma, max_experiences, min_experiences, batch_size, lr)\n",
        "    N = 10000\n",
        "    total_rewards = np.empty(N)\n",
        "    epsilon = 0.99\n",
        "    decay = 0.9999\n",
        "    min_epsilon = 0.1\n",
        "    for n in range(N):\n",
        "        epsilon = max(min_epsilon, epsilon * decay)\n",
        "        total_reward, losses = play_game(env, TrainNet, TargetNet, epsilon, copy_step)\n",
        "        total_rewards[n] = total_reward\n",
        "        avg_rewards = total_rewards[max(0, n - 100):(n + 1)].mean()\n",
        "        with summary_writer.as_default():\n",
        "            tf.summary.scalar('episode reward', total_reward, step=n)\n",
        "            tf.summary.scalar('running avg reward(100)', avg_rewards, step=n)\n",
        "            tf.summary.scalar('average loss)', losses, step=n)\n",
        "        if n % 100 == 0:\n",
        "            print(\"episode:\", n, \"episode reward:\", total_reward, \"eps:\", epsilon, \"avg reward (last 100):\", avg_rewards,\n",
        "                  \"episode loss: \", losses)\n",
        "    print(\"avg reward for last 100 episodes:\", avg_rewards)\n",
        "    \n",
        "    \n",
        "    make_video(env, TrainNet)\n",
        "    env.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F0NXjEULC2tp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':\n",
        "    for i in range(3):\n",
        "        main()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}